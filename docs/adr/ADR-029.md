### Week 3: Optimization (continued)

- [ ] Create materialized views
- [ ] Set up automated view refresh (pg_cron)
- [ ] Implement query performance monitoring
- [ ] Add database indexes based on query patterns
- [ ] Configure connection pooling (PgBouncer)
- [ ] Set up Redis caching for hot queries
- [ ] Enable query result caching
- [ ] Run performance benchmarks (Locust tests)

### Week 4: Monitoring & Alerting

- [ ] Configure Prometheus alerts for database health
- [ ] Set up Grafana dashboards for database metrics
- [ ] Create webhook processing monitoring
- [ ] Implement slow query alerting
- [ ] Configure disk usage alerts
- [ ] Set up replication lag monitoring (if using replicas)
- [ ] Create runbook for common issues
- [ ] Test alert notifications (PagerDuty/Slack)

### Week 5: Backup & Recovery

- [ ] Verify automated backups are running
- [ ] Test point-in-time recovery (PITR)
- [ ] Document recovery procedures
- [ ] Set up S3 lifecycle policies for old backups
- [ ] Test backup restoration (weekly automated test)
- [ ] Create disaster recovery runbook
- [ ] Configure backup monitoring and alerts
- [ ] Validate RPO/RTO targets

### Week 6: Data Migration

- [ ] Backfill historical data (90 days)
- [ ] Run data validation scripts
- [ ] Compare metrics with legacy system (if exists)
- [ ] Enable dual-write mode (if migrating)
- [ ] Monitor data consistency
- [ ] Address any discrepancies
- [ ] Plan cutover timeline
- [ ] Document rollback procedures

### Week 7: Security Hardening

- [ ] Enable SSL/TLS for all connections
- [ ] Rotate database credentials
- [ ] Implement row-level security policies
- [ ] Enable audit logging (pgaudit)
- [ ] Configure WAF rules for API endpoints
- [ ] Perform security scan (Trivy, Snyk)
- [ ] Review IAM policies and roles
- [ ] Document security controls

### Week 8: Production Readiness

- [ ] Conduct load testing (100+ users)
- [ ] Perform chaos engineering tests
- [ ] Validate all monitoring is operational
- [ ] Complete documentation (architecture, runbooks)
- [ ] Train team on operations and troubleshooting
- [ ] Schedule go-live date
- [ ] Prepare rollback plan
- [ ] Conduct final readiness review

-----

## Operational Runbooks

### Runbook 1: High Connection Count

**Symptoms:**

- Alert: “PostgreSQL High Connection Count”
- Dashboard shows >80 active connections
- API latency increasing

**Diagnosis:**

```sql
-- Check current connections
SELECT 
    datname,
    count(*) as connections,
    usename
FROM pg_stat_activity
WHERE state = 'active'
GROUP BY datname, usename
ORDER BY connections DESC;

-- Check long-running queries
SELECT 
    pid,
    now() - pg_stat_activity.query_start AS duration,
    query,
    state
FROM pg_stat_activity
WHERE state != 'idle'
    AND now() - pg_stat_activity.query_start > interval '5 minutes'
ORDER BY duration DESC;
```

**Resolution:**

1. **Immediate**: Kill long-running queries if safe

```sql
SELECT pg_terminate_backend(pid) 
FROM pg_stat_activity 
WHERE pid = <problematic_pid>;
```

1. **Short-term**: Restart PgBouncer to reset connection pool

```bash
kubectl rollout restart deployment/pgbouncer -n fawkes
```

1. **Long-term**: Increase connection limit or optimize queries

```sql
-- Increase max_connections (requires restart)
ALTER SYSTEM SET max_connections = 200;
SELECT pg_reload_conf();
```

**Prevention:**

- Use connection pooling (PgBouncer)
- Set application connection timeout (30 seconds)
- Implement query timeout limits
- Monitor connection patterns

-----

### Runbook 2: Slow Query Performance

**Symptoms:**

- Alert: “PostgreSQL Slow Queries”
- Dashboard query latency >1 second
- User complaints about slow dashboards

**Diagnosis:**

```sql
-- Find slow queries
SELECT 
    query,
    calls,
    mean_exec_time,
    max_exec_time,
    stddev_exec_time
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- 1 second
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Check for missing indexes
SELECT 
    schemaname,
    tablename,
    seq_scan,
    seq_tup_read,
    idx_scan,
    idx_tup_fetch,
    seq_tup_read / seq_scan as avg_seq_tup
FROM pg_stat_user_tables
WHERE seq_scan > 100
    AND seq_tup_read / NULLIF(seq_scan, 0) > 10000
ORDER BY seq_tup_read DESC;

-- Check table bloat
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    n_live_tup,
    n_dead_tup,
    n_dead_tup * 100.0 / NULLIF(n_live_tup, 0) as dead_tuple_percent
FROM pg_stat_user_tables
WHERE n_dead_tup > 1000
ORDER BY n_dead_tup DESC;
```

**Resolution:**

1. **Immediate**: Refresh materialized views

```sql
REFRESH MATERIALIZED VIEW CONCURRENTLY current_flow_metrics;
```

1. **Short-term**: Add missing indexes

```sql
-- Example: Add index for team metrics query
CREATE INDEX CONCURRENTLY idx_flow_items_team_completed_at
ON flow_items(team_id, completed_at)
WHERE status = 'completed';
```

1. **Medium-term**: Vacuum bloated tables

```sql
VACUUM ANALYZE flow_items;
VACUUM ANALYZE stage_transitions;
```

1. **Long-term**: Optimize query or denormalize data

```sql
-- Create aggregated table for better performance
CREATE TABLE team_metrics_summary AS
SELECT 
    team_id,
    date_trunc('day', completed_at) as date,
    count(*) as items_completed,
    percentile_cont(0.5) WITHIN GROUP (ORDER BY flow_time) as p50_flow_time
FROM flow_items
WHERE completed_at IS NOT NULL
GROUP BY team_id, date_trunc('day', completed_at);

CREATE INDEX ON team_metrics_summary(team_id, date DESC);
```

**Prevention:**

- Enable auto-vacuum (should be default)
- Monitor pg_stat_statements regularly
- Set query timeout: `SET statement_timeout = '5s'`
- Use EXPLAIN ANALYZE for new queries

-----

### Runbook 3: Webhook Processing Backlog

**Symptoms:**

- Alert: “Webhook Processing Backlog”
- 100 pending webhook events
- Metrics not updating in real-time

**Diagnosis:**

```sql
-- Check backlog size
SELECT 
    source,
    count(*) as pending_count
FROM webhook_events
WHERE processing_status = 'pending'
GROUP BY source;

-- Check for failures
SELECT 
    source,
    event_type,
    error_message,
    count(*) as failure_count
FROM webhook_events
WHERE processing_status = 'failed'
    AND received_at > NOW() - INTERVAL '1 hour'
GROUP BY source, event_type, error_message
ORDER BY failure_count DESC;

-- Check processing rate
SELECT 
    date_trunc('minute', processed_at) as minute,
    count(*) as processed_count
FROM webhook_events
WHERE processed_at > NOW() - INTERVAL '30 minutes'
GROUP BY date_trunc('minute', processed_at)
ORDER BY minute DESC;
```

**Resolution:**

1. **Immediate**: Scale up webhook processors

```bash
# Increase replicas
kubectl scale deployment/flow-metrics-service --replicas=5 -n fawkes

# Check pod status
kubectl get pods -n fawkes -l app=flow-metrics
```

1. **Short-term**: Reprocess failed events

```python
# Retry failed events
python scripts/reprocess_failed_webhooks.py --hours=1 --limit=100
```

1. **Medium-term**: Identify and fix error patterns

```python
# Common error: missing flow item
# Fix: Create flow item from webhook data
if error_message == 'Flow item not found':
    create_flow_item_from_webhook(webhook_event)
    retry_webhook_processing(webhook_event.id)
```

1. **Long-term**: Optimize webhook processing

```python
# Add batch processing for high-volume events
def process_webhooks_batch(events: List[WebhookEvent]):
    # Process 100 events at once
    with database.transaction():
        for event in events:
            process_webhook(event)
    
    # Commit once instead of 100 times
```

**Prevention:**

- Implement circuit breaker pattern
- Add webhook event TTL (delete after 90 days)
- Monitor webhook sources for unusual volume
- Implement rate limiting on webhook endpoints

-----

### Runbook 4: Disk Space Critical

**Symptoms:**

- Alert: “PostgreSQL Disk Usage High”
- Disk usage >80%
- Write operations may fail soon

**Diagnosis:**

```sql
-- Check database sizes
SELECT 
    datname,
    pg_size_pretty(pg_database_size(datname)) as size
FROM pg_database
ORDER BY pg_database_size(datname) DESC;

-- Check table sizes
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size,
    pg_size_pretty(pg_table_size(schemaname||'.'||tablename)) as table_size,
    pg_size_pretty(pg_indexes_size(schemaname||'.'||tablename)) as indexes_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Check WAL size
SELECT 
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0')) as wal_size;

-- Check bloat
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    n_dead_tup,
    n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0) as dead_percentage
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY n_dead_tup DESC;
```

**Resolution:**

1. **Immediate**: Archive old webhook events to S3

```python
# Archive events older than 30 days
python scripts/archive_old_webhooks.py --days=30 --confirm
```

1. **Short-term**: Vacuum full bloated tables (requires downtime)

```sql
-- Reclaim disk space (use CONCURRENTLY to avoid locks if possible)
VACUUM FULL webhook_events;

-- Or without downtime but slower:
VACUUM ANALYZE webhook_events;
```

1. **Medium-term**: Increase disk size

```bash
# AWS RDS
aws rds modify-db-instance \
    --db-instance-identifier fawkes-flowmetrics \
    --allocated-storage 1000 \
    --apply-immediately
```

1. **Long-term**: Implement data lifecycle management

```python
# Automated cleanup job (daily cron)
def cleanup_old_data():
    # Archive flow items older than 90 days
    old_items = FlowItem.query.filter(
        FlowItem.completed_at < datetime.now() - timedelta(days=90)
    ).all()
    
    for item in old_items:
        # Export to S3
        export_to_s3(item)
        
        # Delete from database
        database.session.delete(item)
    
    database.session.commit()
```

**Prevention:**

- Set up automated archival (90-day policy)
- Monitor disk usage trends
- Alert at 70% (warning) and 80% (critical)
- Enable auto-scaling storage (AWS RDS)

-----

### Runbook 5: Backup Failure

**Symptoms:**

- Alert: “PostgreSQL Backup Failed”
- No recent backup in S3
- WAL archiving failing

**Diagnosis:**

```bash
# Check last successful backup
aws s3 ls s3://fawkes-backups/postgres/flow-metrics/ --recursive | sort | tail -5

# Check WAL archiving status
psql -U postgres -c "SELECT * FROM pg_stat_archiver;"

# Check for disk space issues
df -h /var/lib/postgresql/data

# Check backup logs
kubectl logs -n fawkes postgres-0 | grep -i backup | tail -50
```

**Resolution:**

1. **Immediate**: Manually trigger backup

```bash
# Manual pg_dump
kubectl exec -it postgres-0 -n fawkes -- bash
pg_dump -Fc -U postgres flowmetrics > /tmp/manual_backup_$(date +%Y%m%d).dump

# Upload to S3
aws s3 cp /tmp/manual_backup_*.dump s3://fawkes-backups/postgres/flow-metrics/manual/
```

1. **Short-term**: Fix WAL archiving

```bash
# Check archive_command
psql -U postgres -c "SHOW archive_command;"

# Test manually
/usr/bin/wal-e wal-push /var/lib/postgresql/data/pg_wal/000000010000000000000001

# If failing due to credentials
# Update AWS credentials in Kubernetes secret
kubectl edit secret postgres-backup-credentials -n fawkes
```

1. **Medium-term**: Verify backup script

```bash
# Check cron job status
kubectl get cronjobs -n fawkes
kubectl describe cronjob postgres-backup -n fawkes

# View recent job runs
kubectl get jobs -n fawkes | grep postgres-backup

# Check job logs
kubectl logs job/postgres-backup-12345 -n fawkes
```

1. **Long-term**: Test restore procedure

```bash
# Scheduled restore test (weekly)
python scripts/test_backup_restore.py --backup-date=2025-01-15
```

**Prevention:**

- Monitor backup success rate (should be 100%)
- Test restores monthly
- Alert on failed backups within 1 hour
- Document recovery procedures
- Maintain backup retention policy

-----

## Maintenance Tasks

### Daily

```bash
#!/bin/bash
# daily_maintenance.sh

# Refresh materialized views
psql -U flowmetrics -d flowmetrics -c "REFRESH MATERIALIZED VIEW CONCURRENTLY current_flow_metrics;"

# Update aggregated metrics
python scripts/calculate_daily_team_metrics.py --date=yesterday

# Check for slow queries
psql -U flowmetrics -d flowmetrics -f scripts/check_slow_queries.sql

# Verify backup completed
python scripts/verify_backup.py --date=today

# Archive old webhook events (30+ days)
python scripts/archive_webhooks.py --days=30 --dry-run=false
```

### Weekly

```bash
#!/bin/bash
# weekly_maintenance.sh

# Vacuum analyze all tables
psql -U flowmetrics -d flowmetrics -c "VACUUM ANALYZE;"

# Test backup restore
python scripts/test_restore.py --latest

# Review unused indexes
psql -U flowmetrics -d flowmetrics -f scripts/check_unused_indexes.sql

# Check table bloat
psql -U flowmetrics -d flowmetrics -f scripts/check_table_bloat.sql

# Generate performance report
python scripts/generate_performance_report.py --week

# Update documentation
python scripts/update_metrics_documentation.py
```

### Monthly

```bash
#!/bin/bash
# monthly_maintenance.sh

# Archive completed flow items (90+ days)
python scripts/archive_flow_items.py --days=90

# Optimize database
psql -U flowmetrics -d flowmetrics -c "REINDEX DATABASE flowmetrics;"

# Review and tune configuration
python scripts/analyze_pg_configuration.py --suggest

# Security audit
python scripts/security_audit.py --report

# Cost analysis
python scripts/analyze_costs.py --month=last

# Capacity planning
python scripts/capacity_forecast.py --months=6
```

### Quarterly

```bash
#!/bin/bash
# quarterly_maintenance.sh

# Full database backup verification
python scripts/verify_all_backups.py --quarter

# Disaster recovery drill
python scripts/dr_drill.py --scenario=full_restore

# Performance benchmarking
python scripts/run_benchmarks.py --full

# Schema optimization review
python scripts/analyze_schema.py --recommend

# Update monitoring thresholds
python scripts/tune_alerts.py --analyze=90days

# Review and update runbooks
python scripts/validate_runbooks.py
```

-----

## Appendices

### Appendix A: SQL Schema Migration Scripts

```sql
-- Migration: 001_initial_schema.sql
-- Creates base tables for flow metrics

BEGIN;

CREATE TABLE IF NOT EXISTS teams (
    id VARCHAR(100) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    wip_limit INTEGER DEFAULT 20,
    target_flow_time_hours INTEGER DEFAULT 96,
    target_flow_efficiency_percent INTEGER DEFAULT 40,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
    config JSONB
);

CREATE TABLE IF NOT EXISTS flow_items (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    external_id VARCHAR(255) NOT NULL,
    team_id VARCHAR(100) NOT NULL REFERENCES teams(id),
    item_type VARCHAR(50) NOT NULL,
    title VARCHAR(500),
    description TEXT,
    story_points INTEGER,
    current_stage VARCHAR(50) NOT NULL,
    status VARCHAR(50) NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
    completed_at TIMESTAMP,
    labels JSONB,
    metadata JSONB,
    CONSTRAINT flow_items_external_id_key UNIQUE (external_id, team_id)
);

CREATE TABLE IF NOT EXISTS stage_transitions (
    id BIGSERIAL PRIMARY KEY,
    flow_item_id UUID NOT NULL REFERENCES flow_items(id) ON DELETE CASCADE,
    from_stage VARCHAR(50),
    to_stage VARCHAR(50) NOT NULL,
    transitioned_at TIMESTAMP NOT NULL DEFAULT NOW(),
    duration_seconds INTEGER,
    triggered_by VARCHAR(100),
    event_type VARCHAR(100),
    event_payload JSONB,
    CONSTRAINT stage_transitions_flow_item_stage UNIQUE (flow_item_id, to_stage)
);

CREATE TABLE IF NOT EXISTS stage_metrics (
    id BIGSERIAL PRIMARY KEY,
    flow_item_id UUID NOT NULL REFERENCES flow_items(id) ON DELETE CASCADE,
    stage VARCHAR(50) NOT NULL,
    started_at TIMESTAMP,
    ended_at TIMESTAMP,
    active_time_seconds INTEGER DEFAULT 0,
    wait_time_seconds INTEGER DEFAULT 0,
    flow_efficiency_percent DECIMAL(5,2),
    blockers JSONB,
    metadata JSONB,
    CONSTRAINT stage_metrics_flow_item_stage UNIQUE (flow_item_id, stage)
);

-- Create indexes
CREATE INDEX idx_flow_items_team_status ON flow_items(team_id, status);
CREATE INDEX idx_flow_items_created_at ON flow_items(created_at);
CREATE INDEX idx_flow_items_completed_at ON flow_items(completed_at) WHERE completed_at IS NOT NULL;
CREATE INDEX idx_stage_transitions_flow_item ON stage_transitions(flow_item_id);
CREATE INDEX idx_stage_metrics_flow_item ON stage_metrics(flow_item_id);

COMMIT;
```

### Appendix B: Prometheus Recording Rules

```yaml
# prometheus_rules.yml
# Recording rules for flow metrics aggregation

groups:
  - name: flow_metrics_aggregations
    interval: 60s
    rules:
      # Flow Velocity - Pre-aggregate for performance
      - record: flow_velocity_items_per_week:team
        expr: |
          sum by (team) (
            increase(flow_items_completed_total[7d])
          )
      
      # Flow Time P50 - Pre-calculate percentile
      - record: flow_time_p50_hours:team
        expr: |
          histogram_quantile(0.5, 
            sum by (team, le) (
              rate(flow_time_seconds_bucket[30d])
            )
          ) / 3600
      
      # Flow Time P95
      - record: flow_time_p95_hours:team
        expr: |
          histogram_quantile(0.95, 
            sum by (team, le) (
              rate(flow_time_seconds_bucket[30d])
            )
          ) / 3600
      
      # Stage Wait Time Average
      - record: stage_wait_time_avg_hours:team:stage
        expr: |
          avg by (team, stage) (
            rate(stage_wait_time_seconds_sum[1h])
            /
            rate(stage_wait_time_seconds_count[1h])
          ) / 3600
      
      # Webhook Processing Rate
      - record: webhook_events_processing_rate:source
        expr: |
          sum by (source) (
            rate(webhook_events_processed_total[5m])
          )
      
      # Webhook Processing Error Rate
      - record: webhook_events_error_rate:source
        expr: |
          sum by (source) (
            rate(webhook_events_failed_total[5m])
          )
          /
          sum by (source) (
            rate(webhook_events_received_total[5m])
          )
```

### Appendix C: Database Connection Configuration

```python
# config/database.py
# Production-ready database configuration

from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
import os

# Database URL from environment
DATABASE_URL = os.getenv(
    'DATABASE_URL',
    'postgresql://flowmetrics:password@postgres:5432/flowmetrics'
)

# Connection pool configuration
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,              # Base connection pool size
    max_overflow=40,           # Additional connections when needed
    pool_timeout=30,           # Wait 30s for connection
    pool_recycle=3600,         # Recycle connections after 1 hour
    pool_pre_ping=True,        # Verify connections before use
    echo=False,                # Don't log SQL (use for debugging only)
    echo_pool=False,           # Don't log pool events
    connect_args={
        'connect_timeout': 10,
        'options': '-c statement_timeout=30000'  # 30s query timeout
    }
)

# Session factory
SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)

# Dependency for FastAPI
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

-----

## Conclusion

This ADR documents the decision to use a **hybrid storage architecture** combining **PostgreSQL for relational data** and **Prometheus for time-series metrics**, with **S3 for archival storage**.

### Key Decisions:

1. **PostgreSQL** as primary storage for flow items and detailed event data
1. **Prometheus** for real-time time-series metrics and alerting
1. **S3** for cost-effective long-term archival
1. **Materialized views** for dashboard query performance
1. **Partitioning** for webhook_events table
1. **90-day retention** in hot storage, 1-year in warm, 3+ years in cold

### Trade-offs Accepted:

- Operational complexity of managing two storage systems (PostgreSQL + Prometheus)
- Manual coordination needed between systems (no automatic sync)
- Prometheus retention limited (15 days) - acceptable for MVP

### Future Considerations:

- **TimescaleDB** if consolidation desired
- **ClickHouse** if scale reaches 100M+ flow items
- **Thanos** if Prometheus retention becomes issue
- **Aurora PostgreSQL** if RDS scaling limits reached

### Success Metrics:

- Query latency P95 < 500ms ✅
- Write throughput > 100 events/sec ✅
- Monthly cost < $500 (optimized) ✅
- Data retention 90 days (hot) ✅

This architecture provides a solid foundation for Value Stream Management in Fawkes while maintaining operational simplicity and cost-effectiveness for the MVP phase.

-----

**Status**: Proposed → **Accepted** (pending implementation)

**Next Steps**:

1. Review with platform architecture team
1. Validate cost estimates with finance
1. Begin Week 1 implementation tasks
1. Schedule architecture review after 30 days

**Related ADRs**:

- ADR-006: PostgreSQL for Data Persistence
- ADR-027: Value Stream Management Integration
- ADR-030: Real-Time vs Batch Processing (to be created)

**Last Updated**: 2025-01-15​​​​​​​​​​​​​​​​

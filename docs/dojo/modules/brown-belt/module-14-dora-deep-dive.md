# Fawkes Dojo Module 14: DORA Metrics Deep Dive

## ðŸŽ¯ Module Overview

**Belt Level**: ðŸŸ¤ Brown Belt - Observability & SRE
**Module**: 2 of 4 (Brown Belt)
**Duration**: 60 minutes
**Difficulty**: Advanced
**Prerequisites**:

- Module 2: DORA Metrics (White Belt) review recommended
- Module 13: Observability complete
- Understanding of Prometheus and Grafana
- Familiarity with GitOps workflows

---

## ðŸ“š Learning Objectives

By the end of this module, you will:

1. âœ… Calculate and track all four DORA metrics automatically
2. âœ… Build comprehensive DORA dashboards in Grafana
3. âœ… Implement metric collection across the entire delivery pipeline
4. âœ… Analyze trends and identify improvement opportunities
5. âœ… Benchmark against industry standards
6. âœ… Use metrics to drive platform improvements
7. âœ… Present DORA metrics to leadership effectively

**DORA Capabilities Addressed**:

- âœ“ All 4 Key Metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate)
- âœ“ Monitoring and Observability
- âœ“ Data-Driven Decision Making

---

## ðŸ“– Part 1: DORA Metrics Review & Advanced Concepts

### The Four Key Metrics (Refresher)

| Metric                    | What It Measures                  | Elite Performance |
| ------------------------- | --------------------------------- | ----------------- |
| **Deployment Frequency**  | How often you deploy              | Multiple per day  |
| **Lead Time for Changes** | Commit â†’ Production time          | < 1 hour          |
| **Change Failure Rate**   | % of deployments causing failures | 0-15%             |
| **Mean Time to Restore**  | Time to recover from failure      | < 1 hour          |

### Why These Four?

Research shows these metrics are:

- **Predictive** of organizational performance
- **Balanced** between speed (DF, LT) and stability (CFR, MTTR)
- **Actionable** - teams can directly improve them
- **Universal** - apply across industries and tech stacks

### Advanced DORA Concepts

**1. Metric Correlation**

Metrics don't exist in isolation:

```
High Deployment Frequency
    â†“
Smaller batch sizes
    â†“
Lower Change Failure Rate
    â†“
Faster Lead Time (less code per deploy)
    â†“
Better MTTR (easier to identify issues)
```

**2. Team-Level vs Organization-Level**

- **Team-level**: Track individual team performance
- **Organization-level**: Aggregate across all teams
- **Service-level**: Track per microservice/application

**3. Metric Distributions Matter**

Don't just track averages:

- **P50 (Median)**: Typical case
- **P95**: Worst 5% of cases
- **P99**: Outliers that hurt user experience

**Example**:

```
Lead Time:
- Average: 2 hours
- P50: 30 minutes âœ… (Most deploys are fast)
- P95: 8 hours âŒ (5% take too long - investigate why)
```

---

## ðŸ”¢ Part 2: Calculating DORA Metrics

### Metric 1: Deployment Frequency

**Definition**: Number of deployments per time period

**Calculation**:

```
Deployment Frequency = Total Deployments / Time Period

Example:
- 150 deployments in 30 days
- DF = 150 / 30 = 5 deployments per day âœ… Elite
```

**Data Sources**:

- ArgoCD sync events
- GitOps repository commits
- CI/CD pipeline completions
- Kubernetes deployment events

**Prometheus Query**:

```promql
# Count deployments per day
sum(increase(argocd_app_sync_total{phase="Succeeded"}[1d]))

# Deployment frequency by application
sum(rate(argocd_app_sync_total{phase="Succeeded"}[7d])) by (name) * 86400
```

### Metric 2: Lead Time for Changes

**Definition**: Time from code commit to running in production

**Calculation**:

```
Lead Time = Production Deployment Time - Commit Time

Example:
- Commit: 2025-10-10 14:00:00
- Production: 2025-10-10 14:25:00
- Lead Time: 25 minutes âœ… Elite
```

**Components**:

```
Total Lead Time =
    Code Review Time +
    CI Build Time +
    Test Execution Time +
    Security Scanning Time +
    Artifact Creation Time +
    Deployment Time +
    Validation Time
```

**Data Collection**:

```python
# Webhook receiver for Git commits
@app.route('/webhook/commit', methods=['POST'])
def record_commit():
    commit_sha = request.json['after']
    commit_time = request.json['head_commit']['timestamp']

    # Store in database
    db.store_commit(commit_sha, commit_time)

    return '', 200

# Webhook receiver for deployments
@app.route('/webhook/deploy', methods=['POST'])
def record_deployment():
    commit_sha = request.json['revision']
    deploy_time = datetime.utcnow()

    # Calculate lead time
    commit_time = db.get_commit_time(commit_sha)
    lead_time = (deploy_time - commit_time).total_seconds()

    # Send to Prometheus
    lead_time_histogram.labels(app=app_name).observe(lead_time)

    return '', 200
```

**Prometheus Query**:

```promql
# Average lead time (seconds)
avg(deployment_lead_time_seconds)

# P95 lead time
histogram_quantile(0.95, sum(rate(deployment_lead_time_seconds_bucket[7d])) by (le))

# Lead time by team
avg(deployment_lead_time_seconds) by (team)
```

### Metric 3: Change Failure Rate

**Definition**: Percentage of deployments that result in failure

**Calculation**:

```
CFR = (Failed Deployments / Total Deployments) Ã— 100

Example:
- Total deployments: 100
- Failed deployments: 8
- CFR = (8 / 100) Ã— 100 = 8% âœ… Elite
```

**Defining "Failure"**:

- Deployment rollback within 24 hours
- Incident created within 24 hours of deployment
- Deployment marked as failed in ArgoCD
- Health checks fail post-deployment

**Data Collection**:

```python
def calculate_change_failure_rate(timeframe_hours=24):
    """
    Calculate CFR by correlating deployments with incidents
    """
    deployments = get_deployments(since=timeframe_hours)
    failures = 0

    for deployment in deployments:
        deploy_time = deployment['timestamp']

        # Check for incidents within 24h
        incidents = get_incidents(
            since=deploy_time,
            until=deploy_time + timedelta(hours=24)
        )

        # Check for rollbacks
        rollback = get_rollback(
            deployment_id=deployment['id'],
            since=deploy_time,
            until=deploy_time + timedelta(hours=24)
        )

        if incidents or rollback:
            failures += 1

    cfr = (failures / len(deployments)) * 100 if deployments else 0
    return cfr
```

**Prometheus Query**:

```promql
# Change Failure Rate (%)
sum(deployment_result{status="failed"}) / sum(deployment_result) * 100

# CFR by application
(sum(deployment_result{status="failed"}) by (app) / sum(deployment_result) by (app)) * 100

# CFR trend over time
sum(rate(deployment_result{status="failed"}[7d])) / sum(rate(deployment_result[7d])) * 100
```

### Metric 4: Mean Time to Restore (MTTR)

**Definition**: Average time to recover from a production failure

**Calculation**:

```
MTTR = Total Downtime / Number of Incidents

Example:
- 5 incidents in a month
- Total downtime: 125 minutes
- MTTR = 125 / 5 = 25 minutes âœ… Elite
```

**Data Collection**:

```python
# Incident lifecycle tracking
class Incident:
    def __init__(self, id, severity):
        self.id = id
        self.severity = severity
        self.detected_at = datetime.utcnow()
        self.mitigated_at = None
        self.resolved_at = None

    def mitigate(self):
        """Service restored, but root cause not fixed"""
        self.mitigated_at = datetime.utcnow()
        ttm = (self.mitigated_at - self.detected_at).total_seconds()

        # Time to Mitigate (what we really care about for MTTR)
        mttr_histogram.labels(severity=self.severity).observe(ttm)

    def resolve(self):
        """Root cause fixed, incident closed"""
        self.resolved_at = datetime.utcnow()
        ttr = (self.resolved_at - self.detected_at).total_seconds()

        # Time to Resolve (total incident duration)
        incident_duration_histogram.labels(severity=self.severity).observe(ttr)
```

**Prometheus Query**:

```promql
# Average MTTR (seconds)
avg(incident_duration_seconds)

# MTTR by severity
avg(incident_duration_seconds) by (severity)

# P95 MTTR (captures worst cases)
histogram_quantile(0.95, sum(rate(incident_duration_seconds_bucket[30d])) by (le))

# MTTR trend
avg_over_time(incident_duration_seconds[7d])
```

---

## ðŸ“Š Part 3: Building the Ultimate DORA Dashboard

### Dashboard Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DORA Metrics Dashboard                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Executive Summary (Current vs Target)             â”‚ â”‚
â”‚  â”‚  DF: 5/day (Elite) | LT: 45m (Elite)              â”‚ â”‚
â”‚  â”‚  CFR: 8% (Elite)   | MTTR: 25m (Elite)            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Deployment   â”‚ â”‚ Lead Time    â”‚ â”‚ Change       â”‚   â”‚
â”‚  â”‚ Frequency    â”‚ â”‚ Trend        â”‚ â”‚ Failure Rate â”‚   â”‚
â”‚  â”‚ (Time Series)â”‚ â”‚ (Histogram)  â”‚ â”‚ (Gauge)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MTTR Analysis (by Severity & Trend)             â”‚  â”‚
â”‚  â”‚  SEV1: 15m | SEV2: 1.5h | SEV3: 4h              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Team Comparison (Leaderboard)                    â”‚  â”‚
â”‚  â”‚  Team A: Elite | Team B: High | Team C: Medium  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Improvement Trends (30-day vs 90-day)           â”‚  â”‚
â”‚  â”‚  DF: â†‘15% | LT: â†“20% | CFR: â†“10% | MTTR: â†“25% â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Grafana Dashboard JSON

```json
{
  "dashboard": {
    "title": "DORA Metrics - Platform Performance",
    "tags": ["dora", "metrics", "platform"],
    "timezone": "utc",
    "panels": [
      {
        "id": 1,
        "title": "Deployment Frequency (per day)",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(argocd_app_sync_total{phase='Succeeded'}[7d])) * 86400",
            "legendFormat": "Deployments/Day"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                { "value": 0, "color": "red" },
                { "value": 0.1, "color": "yellow" },
                { "value": 1, "color": "green" }
              ]
            },
            "mappings": [],
            "unit": "short"
          }
        },
        "gridPos": { "h": 8, "w": 6, "x": 0, "y": 0 }
      },
      {
        "id": 2,
        "title": "Lead Time for Changes (P95)",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(deployment_lead_time_seconds_bucket[7d])) by (le)) / 3600",
            "legendFormat": "P95 Hours"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 1, "color": "yellow" },
                { "value": 24, "color": "red" }
              ]
            },
            "unit": "h"
          }
        },
        "gridPos": { "h": 8, "w": 6, "x": 6, "y": 0 }
      },
      {
        "id": 3,
        "title": "Change Failure Rate",
        "type": "gauge",
        "targets": [
          {
            "expr": "sum(rate(deployment_result{status='failed'}[7d])) / sum(rate(deployment_result[7d])) * 100"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 15, "color": "yellow" },
                { "value": 30, "color": "red" }
              ]
            },
            "max": 100,
            "unit": "percent"
          }
        },
        "gridPos": { "h": 8, "w": 6, "x": 12, "y": 0 }
      },
      {
        "id": 4,
        "title": "Mean Time to Restore",
        "type": "stat",
        "targets": [
          {
            "expr": "avg(incident_duration_seconds) / 60",
            "legendFormat": "Avg Minutes"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 60, "color": "yellow" },
                { "value": 1440, "color": "red" }
              ]
            },
            "unit": "m"
          }
        },
        "gridPos": { "h": 8, "w": 6, "x": 18, "y": 0 }
      },
      {
        "id": 5,
        "title": "Deployment Frequency Trend",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(argocd_app_sync_total{phase='Succeeded'}[1d])) by (name) * 86400",
            "legendFormat": "{{name}}"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 }
      },
      {
        "id": 6,
        "title": "Lead Time Distribution",
        "type": "heatmap",
        "targets": [
          {
            "expr": "sum(increase(deployment_lead_time_seconds_bucket[1h])) by (le)",
            "format": "heatmap",
            "legendFormat": "{{le}}"
          }
        ],
        "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 }
      }
    ]
  }
}
```

---

## ðŸŽ¯ Part 4: Hands-On Lab - Complete DORA Implementation

### Objective

Implement end-to-end DORA metrics collection and visualization for Fawkes platform.

### Step 1: Deploy DORA Metrics Collector

Create `dora-collector.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dora-collector
  namespace: dojo-metrics
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dora-collector
  template:
    metadata:
      labels:
        app: dora-collector
    spec:
      containers:
        - name: collector
          image: fawkes/dora-collector:v1.0
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 9090
              name: metrics
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: dora-db-credentials
                  key: url
            - name: PROMETHEUS_URL
              value: "http://prometheus:9090"
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
              memory: "256Mi"
              cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: dora-collector
  namespace: dojo-metrics
spec:
  selector:
    app: dora-collector
  ports:
    - name: http
      port: 80
      targetPort: 8080
    - name: metrics
      port: 9090
      targetPort: 9090
---
apiVersion: monitoring.coreos.io/v1
kind: ServiceMonitor
metadata:
  name: dora-collector
  namespace: dojo-metrics
spec:
  selector:
    matchLabels:
      app: dora-collector
  endpoints:
    - port: metrics
      interval: 30s
```

### Step 2: Configure Webhooks

**ArgoCD Webhook** (for deployments):

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-notifications-cm
  namespace: argocd
data:
  service.webhook.dora: |
    url: http://dora-collector.dojo-metrics/webhook/deploy
    headers:
    - name: Content-Type
      value: application/json

  trigger.on-deployed: |
    - when: app.status.operationState.phase in ['Succeeded']
      send: [dora-deploy-succeeded]
    - when: app.status.operationState.phase in ['Failed']
      send: [dora-deploy-failed]

  template.dora-deploy-succeeded: |
    webhook:
      dora:
        method: POST
        body: |
          {
            "event": "deployment",
            "status": "success",
            "app": "{{.app.metadata.name}}",
            "revision": "{{.app.status.sync.revision}}",
            "timestamp": "{{.app.status.operationState.finishedAt}}"
          }

  template.dora-deploy-failed: |
    webhook:
      dora:
        method: POST
        body: |
          {
            "event": "deployment",
            "status": "failed",
            "app": "{{.app.metadata.name}}",
            "revision": "{{.app.status.sync.revision}}",
            "timestamp": "{{.app.status.operationState.finishedAt}}"
          }
```

**Git Webhook** (for commits):

```bash
# Add webhook to GitHub repository
curl -X POST \
  -H "Authorization: token ${GITHUB_TOKEN}" \
  -H "Content-Type: application/json" \
  https://api.github.com/repos/myorg/myapp/hooks \
  -d '{
    "name": "web",
    "active": true,
    "events": ["push"],
    "config": {
      "url": "https://dora-collector.fawkes.io/webhook/commit",
      "content_type": "json"
    }
  }'
```

### Step 3: Create Grafana Dashboard

```bash
# Import dashboard via API
curl -X POST \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${GRAFANA_API_KEY}" \
  http://grafana:3000/api/dashboards/db \
  -d @dora-dashboard.json

# Or import via UI:
# Grafana â†’ Dashboards â†’ Import â†’ Upload dora-dashboard.json
```

### Step 4: Validate Data Collection

```bash
# Check if metrics are being collected
kubectl port-forward -n dojo-metrics svc/dora-collector 9090:9090

# Query Prometheus
curl "http://localhost:9090/metrics" | grep deployment

# Expected output:
# deployment_frequency_total{app="myapp"} 150
# deployment_lead_time_seconds_sum{app="myapp"} 450000
# deployment_lead_time_seconds_count{app="myapp"} 150
# deployment_result{app="myapp",status="success"} 142
# deployment_result{app="myapp",status="failed"} 8
```

### Step 5: Analyze Your Metrics

Access Grafana dashboard and analyze:

1. **Deployment Frequency**: Are you deploying daily? Multiple times per day?
2. **Lead Time**: What's your P95? Where are the bottlenecks?
3. **CFR**: Which deployments are failing? Common patterns?
4. **MTTR**: How quickly do you recover? Can you automate more?

---

## ðŸ“ˆ Part 5: Advanced Analysis Techniques

### Trend Analysis

**Week-over-week comparison**:

```promql
# Current week deployment frequency
sum(rate(argocd_app_sync_total{phase="Succeeded"}[7d])) * 86400

# Previous week
sum(rate(argocd_app_sync_total{phase="Succeeded"}[7d] offset 7d)) * 86400

# % change
(
  sum(rate(argocd_app_sync_total{phase="Succeeded"}[7d]))
  -
  sum(rate(argocd_app_sync_total{phase="Succeeded"}[7d] offset 7d))
)
/
sum(rate(argocd_app_sync_total{phase="Succeeded"}[7d] offset 7d))
* 100
```

### Correlation Analysis

**Does higher deployment frequency correlate with lower CFR?**

```python
import pandas as pd
from scipy.stats import pearsonr

# Fetch data
df = pd.DataFrame({
    'team': teams,
    'deployment_freq': [get_deployment_freq(t) for t in teams],
    'cfr': [get_cfr(t) for t in teams]
})

# Calculate correlation
correlation, p_value = pearsonr(df['deployment_freq'], df['cfr'])

print(f"Correlation: {correlation:.2f}")
print(f"P-value: {p_value:.4f}")

# Expected: Negative correlation (higher DF â†’ lower CFR)
```

### Identifying Bottlenecks

**Lead time breakdown**:

```promql
# Time in each stage
sum(ci_stage_duration_seconds{stage="build"}) by (app)
sum(ci_stage_duration_seconds{stage="test"}) by (app)
sum(ci_stage_duration_seconds{stage="scan"}) by (app)
sum(ci_stage_duration_seconds{stage="deploy"}) by (app)
```

Create waterfall chart to visualize:

```
Commit â†’ Build (3m) â†’ Test (5m) â†’ Scan (2m) â†’ Deploy (1m) = 11m total
         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ        â–ˆâ–ˆ

Bottleneck: Testing takes 45% of lead time
Action: Parallelize tests or optimize slow tests
```

---

## ðŸ’ª Part 6: Driving Improvements with Data

### Improvement Framework

**1. Measure Current State**

```
Current Performance (Last 30 days):
- DF: 3 per day (High)
- LT: 2 hours (High)
- CFR: 12% (Elite)
- MTTR: 45 minutes (Elite)

Overall: High Performer
```

**2. Set Targets**

```
3-Month Goals:
- DF: 5 per day (Elite) - â†‘67%
- LT: 1 hour (Elite) - â†“50%
- CFR: <10% (Elite) - â†“17%
- MTTR: <30 min (Elite) - â†“33%
```

**3. Identify Bottlenecks**

```
Lead Time Breakdown:
- Code Review: 45 min (38%)
- CI Build: 15 min (13%)
- Testing: 35 min (29%)
- Deployment: 25 min (21%)

Biggest opportunity: Code Review (38% of lead time)
```

**4. Implement Changes**

```
Actions:
1. Reduce PR size (enforce <300 lines)
2. Pair programming for complex changes (faster review)
3. Async code review tools (remove scheduling overhead)
4. Auto-approve trivial changes (docs, formatting)

Expected Impact: Reduce code review time by 50% (22min savings)
New Lead Time: 1h 5min â†’ Target not quite met, but significant progress
```

**5. Measure Impact**

```
After 30 days:
- DF: 4.5 per day âœ… (On track)
- LT: 1h 15min âš ï¸ (Close to target)
- CFR: 9% âœ… (Target met!)
- MTTR: 28 min âœ… (Target exceeded!)

Continue iteration...
```

---

## ðŸŽ“ Part 7: Knowledge Check

### Quiz Questions

1. **What does P95 lead time represent?**

   - [ ] Average lead time
   - [ ] Fastest lead time
   - [x] 95% of deployments complete within this time
   - [ ] Slowest lead time

2. **How do you calculate Change Failure Rate?**

   - [ ] Failed deployments Ã— 100
   - [x] (Failed deployments / Total deployments) Ã— 100
   - [ ] Total deployments / Failed deployments
   - [ ] Failed deployments / Successful deployments

3. **What's the Elite benchmark for Deployment Frequency?**

   - [ ] Once per week
   - [ ] Once per day
   - [x] Multiple times per day
   - [ ] Continuous deployment

4. **What should MTTR measure?**

   - [ ] Time to write code
   - [ ] Time to test
   - [x] Time to restore service after incident
   - [ ] Time to deploy

5. **Why track team-level DORA metrics separately?**

   - [ ] To rank teams
   - [x] To identify improvement opportunities specific to each team
   - [ ] To punish low performers
   - [ ] It's not necessary

6. **What does high DF + low CFR indicate?**

   - [ ] Luck
   - [x] Mature CI/CD with good quality gates
   - [ ] Metrics are broken
   - [ ] Too much testing

7. **How often should you review DORA metrics?**

   - [ ] Annually
   - [ ] When problems occur
   - [x] Weekly or monthly for trends
   - [ ] Once after implementation

8. **What's a good first step to improve lead time?**
   - [ ] Skip testing
   - [ ] Deploy less frequently
   - [x] Identify and optimize the slowest stage
   - [ ] Hire more people

**Answers**: 1-C, 2-B, 3-C, 4-C, 5-B, 6-B, 7-C, 8-C

---

## ðŸŽ¯ Part 8: Module Summary & Next Steps

### What You Learned

âœ… **Advanced Calculation**: All 4 metrics with distributions
âœ… **Data Collection**: Webhooks, Prometheus, automation
âœ… **Dashboards**: Comprehensive Grafana visualizations
âœ… **Analysis**: Trends, correlations, bottlenecks
âœ… **Improvement**: Data-driven optimization framework
âœ… **Presentation**: Communicate metrics to leadership

### DORA Capabilities Achieved

- âœ… **All 4 Key Metrics**: Automated collection and tracking
- âœ… **Monitoring**: Real-time visibility into delivery performance
- âœ… **Data-Driven**: Metrics inform platform improvements

### Key Takeaways

1. **Metrics must be actionable** - If you can't improve it, don't measure it
2. **Track distributions, not just averages** - P95/P99 reveal user experience
3. **Compare teams carefully** - Context matters (legacy vs greenfield)
4. **Automate collection** - Manual tracking doesn't scale
5. **Review regularly** - Weekly trends reveal improvement opportunities

### Real-World Impact

"After implementing comprehensive DORA tracking:

- **Identified bottleneck**: Code review was 40% of lead time
- **Action**: Reduced PR size, added auto-approval for trivial changes
- **Result**: Lead time decreased 35% in 60 days
- **Visibility**: Leadership now tracks metrics quarterly
- **Culture**: Teams compete (healthily) to improve metrics

Metrics transformed from vanity to value."

- _Engineering Director, Tech Company_

---

## ðŸ“š Additional Resources

### Tools

- [Four Keys](https://github.com/dora-team/fourkeys) - DORA metrics collection
- [Sleuth](https://www.sleuth.io/) - DORA tracking SaaS
- [LinearB](https://linearb.io/) - Engineering intelligence

### Reading

- [DORA State of DevOps Reports](https://dora.dev/research/)
- [Accelerate](https://itrevolution.com/accelerate-book/) - The research behind DORA
- [DORA Metrics Guide](https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance)

---

## ðŸ… Module Completion

### Assessment Checklist

- [ ] **Conceptual Understanding**

  - [ ] Calculate all 4 metrics correctly
  - [ ] Understand P50/P95/P99 distributions
  - [ ] Explain metric correlations

- [ ] **Practical Skills**
  - [ ] Deploy DORA collector
  - [ ] Configure webhooks
  - [ ] Build Grafana dashboard
  - [ ] Analyze trends
  -
